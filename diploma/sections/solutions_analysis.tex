\documentclass[times,specification,annotation]{itmo-student-thesis}
\usepackage{fancyhdr}

%% Опции пакета:
%% - specification - если есть, генерируется задание, иначе не генерируется
%% - annotation - если есть, генерируется аннотация, иначе не генерируется
%% - times - делает все шрифтом Times New Roman, собирается с помощью xelatex
%% - languages={...} - устанавливает перечень используемых языков. По умолчанию это {english,russian}.
%%                     Последний из языков определяет текст основного документа.

%% Делает запятую в формулах более интеллектуальной, например:
%% $1,5x$ будет читаться как полтора икса, а не один запятая пять иксов.
%% Однако если написать $1, 5x$, то все будет как прежде.
\usepackage{icomma}

%% Один из пакетов, позволяющий делать таблицы на всю ширину текста.
\usepackage{tabularx}

%% Данные пакеты необязательны к использованию в бакалаврских/магистерских
%% Они нужны для иллюстративных целей
%% Начало
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{filecontents}
%% Конец

%% Указываем файл с библиографией.
\addbibresource{bachelor-thesis.bib}

\begin{document}

\chapter{Анализ существующих решений}
Существуют различные методы токенизации изображения (то есть преобразования его в набор плотных векторов).

\section{Токенизация изображений Vision Transformer}
Процесс токенизации изображений в Vision Transformer подразумевает разбиение изображения на патчи.

Данный подход обладает своими преимуществами и недостатками.

\begin{itemize}
    \item По сравнению с методом попиксельной токенизации, предложенной в Image Tranformer, использование патчей вместо пикселей позволяет сократить длину входной последовательности в $s_p^2$ раз при условии разбиения на патчи размером $(s_p, s_p)$. Например, для изображения $(224, 224)$ пикселей, количество элементов входной последовательности снижается с $224^2 = 50 176$ до $ \Big(\dfrac{224}{16}\Big)^2 = 196$.
    \item Поскольку процесс разбиения изображения на патчи $(s_p, s_p)$ с последующей проекцией в пространство некоторой внутренней размерности трансформера можно заменить одним сверточным слоем с размером ядра свертки $s_p$ и шагом $s_p$, можно говорить о сохранении информации о простейших локальных структурах (например, о границах объектов и их текстурах) внутри одного патча.
    \item Поскольку архитектура ViT почти полностью повторяет архитектуру трансформера-кодировщика, токены выходной последовательности могут быть совместимы с текстовыми токенами после, например, линейной проекции или даже без нее. Данное свойство очень важно, поскольку используется в визуальных языковых моделях (VLM), например, в модели LLaVA\cite{llava}, и в таких моделях, как CLIP\cite{clip}, и позволяет связать текстовую модальность и модальность изображений при помощи общей архитектуры трансформера.
\end{itemize}

Однако, несмотря на ряд преимуществ, этот метод токенизации несовершенен. Некоторые из недостатков могут быть критическими при разработке современных моделей. Например, при разработке визуальных языковых моделей, где ставятся высокие требования к длине входной последовательности. Данный метод токенизации имеет следующие недостатки:

\begin{itemize}
    \item Несмотря на сокращение длины входной последовательности в $p_s^2$ раз по сравнению с попиксельной токенизацией, количество патчей, а соответственно токенов, все еще линейно зависит от количества пикселей изображения. Это значит, что при линейном увеличении размера изображения, количество требуемых ресурсов будет расти квадратично. Это может быть критичным при обработке изображений большой размерности, нескольких изображений одновременно (для учета общего контекста этих изображений) или обработки видео.

    \item Поскольку сетка разбиения на патчи фиксирована, наиболее вероятно, что она будет ``разрезать`` объекты, проходя через их границы. В работе \cite{vit_patch_lack} прямо указано, что классическая токенизация Vision Transformer приводит к уничтожению семантики объектов. Из-за этого происходит потеря локальной целостности различных объектов. Например, лицо человека, попавшее на стык нескольких патчей, будет обработано фрагментарно. Данная проблема негативно влияет на понимание моделью локального контекста.
    
    \item Из-за фиксированного размера патчей, классические модели Vision Transformer плохо адаптируются к изображениям разной размерности. Проблеме адаптации Vision Transformer для обработки изображений разного размера посвящена работа \cite{vitar}.   

    \item Из-за метода токенизации и фиксированной сетки разбиения, однотонные и семантически богатые патчи требуют одинакого количества вычислений на обработку \cite{vit_patch_slimming}. Это приводит к нерациональному использованию ресурсов.
\end{itemize}

\section{Токенизация изображений на уровне субъектов}

Для решения проблемы траты одинаковых ресурсов на обработку однотонных и семантически богатых патчей был предложен метод токенизации изображений на уровне субъобъектов. 

В статье \cite{subobject_tokenization} предложен метод токенизации изображения на основе субобъектов (семантически значимых сегментов изображения). Вместо разбиения изображения на фиксированные патчи, авторы используют модель сегментации DirectSAM, спроектированную ими самостоятельно, для выделения структурно-осмысленных частей объектов. С помощью модели SeqAE, также предложенной в этой работе, эти сегменты преобразуются в плотные векторы.

Авторы заявляют о ускорении обучения при использовании данного метода токенизации. Также они указывают, что в отличие от ViT, длина последовательности зависит не от размера изображения, а от количества объектов на нем. 

Однако это преимущество не имеет смысла, поскольку для работы такого подхода необходимо использовать две дополнительные модели для подготовки токенов, что значительно повышает требуемые ресурсы во время работы. По этой причине, этот подход не будет рассмотрен далее, поскольку из-за своего главного недостатка делает невозможным решение задачи уменьшения требуемых для работы модели-трансформера ресурсов.

\section{Токенизация изображений на основе Вейвлет-разложения}

Вейвлет-преобразование --- это инструмент для анализа изображений и сигналов. Данное преобразование выполняет разложение изображения на низкочастотную и высокочастотные составляющие. При этом, низкочастотная составляющая сохраняет большую часть информации изображения. При каждом таком преобразовании требуемый для хранения изображения объем памяти уменьшается \cite{mallat_wavelet}.

В статье \cite{wavelet_tokenization} предложен алгоритм токенизации изображений на основе Вейвлет-разложения:

\begin{itemize}
    \item Выполняется преобразование RBG в YCbCr.
    \item Над полученным изображением в YCbCr формате выполняется Вейвлет-преобразование. Изображение разбивается на уровни детализации (LL, LH, HL и HH). Каждый уровень соответствует разным частотным компонентам.
    \item Малозначимые высокочастотные элементы обнуляются. Это позволяет сократить объем данных без существенной потери информации.
    \item Оставшиеся составляющие объединяются в тензор $W$, в котором каждый вектор соответствует локальной области пикселей.
    \item Векторы тензора $W$ из пиксельного пространства с помощью линейной проекции проецируются в пространство меньшей размерности 
    $$
    E = W Q,
    $$
    где $Q$ - матрица проекции, а $E$ --- проекция $W$ в ``семантическое`` пространство.
\end{itemize}

К преимуществам данного подхода можно отнести:

\begin{itemize}
    \item Возможность настройки детализации изображения перед его обработкой. Это позволяет значительно уменьшить длину входной последовательности и работать даже с изображениями в сверхвысоком ($N > 2048$) качестве.
    \item Устойчивость к адверсариальным атакам. Поскольку малозначимые высокочастотные составляющие обнуляются, значительно снижается влияние шумов.
    \item За счет неоднородности в высокочастотных составляющих модель фокусируется на информативных областях, пропуская малоинформативные.
\end{itemize}

Однако, у данного подхода также есть много проблем:

\begin{itemize}
    \item Зависимость производительности от гиперпараметров (таких как уровень детализации и порог округления высокочастотных признаков). Неверный выбор гиперпараметров может привести к потере слишком важных деталей или недостаточному сжатию. Таким образом для настройки гиперпараметров требуется трудоемкая работа.
    \item Вейвлет-ядра фиксированы и не способны подстраиваться под данные, что уменьшает адаптивность модели.
    \item Многократное вейвлет-преобразование требует дополнительных ресурсов.
\end{itemize}

Несмотря на имеющиеся проблемы, методы токенизации на основе Вейвлет-преобразования получают все большее распространение. В работе \cite{wavelet_autoregression} даже предложен способ авторегрессионной генерации моделью-трансформером на основе вейвлет-разложения. 

\s\chapterconclusion
Несмотря на появление новых методов, область токенизации изображений все еще требует тчательного изучения, поскольку предлагаемые методы все еще не способны побороть алгоритм токенизации, основанный на разбиении изображений на патчи. Метод токенизации на основе Вейвлет-разложения хоть и может быть адаптирован для других задач, однако изначально предназначен только для генерации изображений, а метод токенизации изображений на основе субъектов применим в очень редких случаях, поскольку требует для своей работы две дополнительные модели с большим количеством параметров, что значительно повышает требуемые для работы алгоритма ресурсы. 

\end{document}